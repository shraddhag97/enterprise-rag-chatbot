
# Enterprise RAG Chatbot ðŸ¤–

An enterprise-grade conversational chatbot built using **Retrieval Augmented Generation (RAG)**.  
The system provides **document-grounded, non-hallucinating answers** with session-based conversational memory.

---

## âœ¨ Key Features

- ðŸ” **Retrieval Augmented Generation (RAG)**
  - Answers are generated strictly from provided documents
  - No hallucinated responses
- ðŸ§  **Session-based conversational memory**
  - Supports follow-up questions using `session_id`
- âš¡ **FastAPI backend**
  - Clean, modular, API-first design
- ðŸ’¬ **Streamlit chat UI**
  - Real-time chat interface
  - Automatic session handling
- ðŸ§± **Vector search using FAISS**
  - Efficient similarity search over document embeddings
- ðŸ” **Secure by design**
  - Environment variables for secrets
  - `.env` and `.venv` excluded from Git

---

## ðŸ—ï¸ System Architecture

```

User
â”‚
â”‚  (chat input)
â–¼
Streamlit UI
â”‚
â”‚  POST /chat
â–¼
FastAPI Backend
â”‚
â”œâ”€â”€ Session Memory
â”‚
â”œâ”€â”€ RAG Pipeline
â”‚     â”œâ”€â”€ Document Loader
â”‚     â”œâ”€â”€ Embedding Generator (OpenAI)
â”‚     â”œâ”€â”€ FAISS Vector Store
â”‚     â””â”€â”€ Context Retrieval
â”‚
â””â”€â”€ LLM Response Generator
â”‚
â–¼
Grounded Answer

```

**Key principle:**  
> The LLM is never allowed to answer outside the retrieved document context.

---

## ðŸ“‚ Project Structure

```

enterprise-rag-chatbot/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api.py                # FastAPI entry point
â”‚   â”œâ”€â”€ config.py             # Environment & app config
â”‚   â”œâ”€â”€ schemas/              # Request/response models
â”‚   â”œâ”€â”€ chatbot/
â”‚   â”‚   â””â”€â”€ memory.py         # Session-based memory
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ loader.py         # Document loading
â”‚       â”œâ”€â”€ embeddings.py     # Embedding generation
â”‚       â”œâ”€â”€ vector_store.py   # FAISS vector store
â”‚       â””â”€â”€ rag_pipeline.py   # End-to-end RAG logic
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ docs/
â”‚       â””â”€â”€ sample.txt        # Example knowledge source
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ chat_ui.py            # Streamlit chat interface
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

````

---

## âš™ï¸ Tech Stack

- **Backend:** FastAPI
- **Frontend:** Streamlit
- **LLM:** OpenAI (pluggable: Azure / DeepSeek compatible)
- **Embeddings:** OpenAI Embeddings
- **Vector DB:** FAISS
- **Language:** Python 3.11

---

## â–¶ï¸ How to Run Locally

### 1ï¸âƒ£ Create virtual environment
```bash
python3.11 -m venv .venv
source .venv/bin/activate
````

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configure environment variables

Create a `.env` file:

```env
OPENAI_API_KEY=your_api_key_here
```

### 4ï¸âƒ£ Start backend

```bash
uvicorn app.api:app
```

Backend runs at:

```
http://127.0.0.1:8000
```

### 5ï¸âƒ£ Start Streamlit UI

```bash
streamlit run ui/chat_ui.py
```

UI runs at:

```
http://localhost:8501
```

---

## ðŸ§ª Example Usage

1. Ask:

   > *What is Enterprise RAG Chatbot?*

2. Follow up:

   > *Is it trained on company data?*

The chatbot:

* Uses memory to understand context
* Answers **only** from documents
* Responds safely if information is missing

---

## ðŸ§  Design Highlights (Interview-Ready)

* RAG implemented **from scratch** (no LangChain dependency)
* Strict grounding enforcement (`"I don't know based on provided documents"`)
* Clean separation of UI, API, and AI logic
* Easily extensible to:

  * Azure OpenAI
  * S3 / Blob document sources
  * Redis-based memory
  * LangChain abstractions

---

## ðŸš€ Future Enhancements

* Document chunking & metadata filtering
* Conversation summarization memory
* Multi-document citation support
* Authentication & role-based access
* Deployment on AWS / Azure

---

## ðŸ“Œ Disclaimer

This project is for educational and portfolio purposes.
No proprietary or confidential data is included.

````

